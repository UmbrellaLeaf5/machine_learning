{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение и принцип работы\n",
    "Построим модель, используя **Линейную регрессию**.\n",
    "\n",
    "Линейная регрессия — это алгоритм **обучения с учителем**, используемый, когда целевая/зависимая переменная является действительным числом. При линейной регресии устанавливается связь между зависимой переменной $y$ и одной или несколькими независимыми переменными $x$, используя линию наилучшего соответствия. Линейная регрессия работает по принципу наименьших квадратов $(OLS)$ и среднеквадратической ошибки $(MSE)$. В статистике OLS — это метод оценки неизвестных параметров функции линейной регрессии, цель которого — минимизировать сумму квадратов разностей между наблюдаемыми зависимыми переменными в заданном наборе данных и значениями, предсказанными функцией линейной регрессии.\n",
    "\n",
    "## Представление гипотезы\n",
    "\n",
    "Мы будем использовать $\\mathbf{x_i}$ для обозначения независимой переменной, а $\\mathbf{y_i}$ — для обозначения зависимой переменной. Пара $\\mathbf{(x_i,y_i)}$ называется обучающим примером. Индекс $\\mathbf{i}$ в обозначении — это просто индекс в обучающем наборе. У нас есть обучающий пример $\\mathbf{m}$, тогда $\\mathbf{i = 1,2,3,...m}$.\n",
    "\n",
    "Цель обучения с учителем — обучить *функцию гипотезы $\\mathbf{h}$* для заданного обучающего набора, которую можно использовать для оценки $\\mathbf{y}$ на основе $\\mathbf{x}$. Таким образом, функция гипотезы представлена ​​как\n",
    "\n",
    "$$\\mathbf{ h_\\theta(x_{i}) = \\theta_0 + \\theta_1x_i }$$\n",
    "$\\mathbf{\\theta_0,\\theta_1}$ являются параметрами гипотезы. Это уравнение для **простой/одномерной линейной регрессии**.\n",
    "\n",
    "<img src=data/lr2.png></img> \n",
    "\n",
    "Для **множественной линейной регрессии**, где присутствует более одной независимой переменной, мы будем использовать $\\mathbf{x_{ij}}$ для обозначения независимой переменной и $\\mathbf{y_{i}}$ для обозначения зависимой переменной. Если независимая переменная $\\mathbf{n}$, то $\\mathbf{j=1,2,3 ..... n}$. Функция гипотезы, представленная как\n",
    "\n",
    "$$\\mathbf{h_\\theta(x_{i}) = \\theta_0 + \\theta_1x_{i1} + \\theta_2 x_{i2} + ..... \\theta_j x_{ij} ...... \\theta_n x_{mn} }$$\n",
    "$\\mathbf{\\theta_0,\\theta_1,....\\theta_j....\\theta_n }$ — параметр гипотезы,\n",
    "$\\mathbf{m}$ Количество обучающих примеров,\n",
    "$\\mathbf{n}$ Номер независимой переменной,\n",
    "$\\mathbf{x_{ij}}$ — это $\\mathbf{i^{th}}$ пример обучения функции $\\mathbf{j^{th}}$.\n",
    "\n",
    "<img src=data/lr3.png></img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предскажем зарплату ($y$) по опыту работы ($x_1$) и возрасту ($x_2$):\n",
    "\n",
    "$$\n",
    "\\hat{y} = 20{,}000 + 3{,}000 \\cdot x_1 + 500 \\cdot x_2\n",
    "$$\n",
    "\n",
    "* Опыт работы на год больше → зарплата выше на 3000.\n",
    "* Возраст на год больше → зарплата выше на 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по картам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную\n",
    "\n",
    "<img src=data/ya_linear1.png></img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первичный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas  as pd #Data manipulation\n",
    "import numpy as np #Data manipulation\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "import seaborn as sns #Visualization\n",
    "plt.rcParams['figure.figsize'] = [8,5]\n",
    "plt.rcParams['font.size'] =14\n",
    "plt.rcParams['font.weight']= 'bold'\n",
    "#plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "#path ='dataset/'\n",
    "path = 'data/insurance/'\n",
    "df = pd.read_csv(path+'insurance.csv')\n",
    "print('\\nNumber of rows and columns in the data set: ',df.shape)\n",
    "print('')\n",
    "\n",
    "#Lets look into top few rows and columns in the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, есть обучающий выбор с $\\mathbf{m=1338}$ примерами, в каждом из которых переменных - $\\mathbf{n=7}$. Целевая переменная здесь — это расходы, а оставшиеся шесть переменных, такие как возраст, пол, ИМТ, дети, курение и регион, являются признаками. Целевая функция выглядит следующим образом:\n",
    "\n",
    "$$\\mathbf{ h_\\theta(x_{i}) = \\theta_0+\\theta_1 age + \\theta_2 sex + \\theta_3 bmi + \\theta_4 children + \\theta_5 smoker + \\theta_6 region }$$\n",
    "\n",
    "Для такой функции объект выборки будет слудующим.  \n",
    "If $\\mathbf{i=0}$ then \n",
    "\n",
    "$$\\mathbf{h_\\theta(x_{0}) = \\theta_0+\\theta_1 19 + \\theta_2 female + \\theta_3 27.900 + \\theta_4 0 + \\theta_5 yes + \\theta_6 southwest}$$\n",
    "\n",
    "If $\\mathbf{i=2}$ then $$\\mathbf{h_\\theta(x_{2}) = \\theta_0+\\theta_1 28 + \\theta_2 male + \\theta_3 33.000 + \\theta_4 3 + \\theta_5 no + \\theta_6 northwest}$$ \n",
    "$$\\mathbf{y_3 = 4449.46200}$$\n",
    "\n",
    "$$\\mathbf{x_1 = \\left(\\begin{matrix} x_{11} & x_{12} & x_{13} & x_{14} & x_{15} & x_{16}\\end{matrix}\\right) = \\left(\\begin{matrix} 19 & female & 27.900 & 1 & no & northwest\\end{matrix}\\right) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричная формулировка\n",
    "\n",
    "В общем случае вектор можно записать как $$ \\mathbf{ x_{ij}} = \\left( \\begin{smallmatrix} \\mathbf{x_{i1}} & \\mathbf{x_{i2}} &.&.&.& \\mathbf{x_{in}} \\end{smallmatrix} \\right)$$\n",
    "\n",
    "Теперь мы объединяем все доступные отдельные векторы в одну входную матрицу размера $(m,n)$ и обозначаем её как входная матрица $\\mathbf{X}$, которая состоит из всех обучающих примеров,\n",
    "$$\\mathbf{X} = \\left( \\begin{smallmatrix} x_{11} & x_{12} &.&.&.&.& x_{1n}\\\\\n",
    "x_{21} & x_{22} &.&.&.&.& x_{2n}\\\\\n",
    "x_{31} & x_{32} &.&.&.&.& x_{3n}\\\\\n",
    ".&.&.&. &.&.&.& \\\\\n",
    ".&.&.&. &.&.&.& \\\\\n",
    "x_{m1} & x_{m2} &.&.&.&.&. x_{mn}\\\\\n",
    "\\end{smallmatrix} \\right)_{(m,n)}$$\n",
    "\n",
    "Представим параметр функции и зависимую переменную в векторной форме как:\n",
    "$$\\theta = \\left (\\begin{matrix} \\theta_0 \\\\ \\theta_1 \\\\ .\\\\.\\\\ \\theta_j\\\\.\\\\.\\\\ \\theta_n \\end {matrix}\\right)_{(n+1,1)}\n",
    "\\mathbf{ y } = \\left (\\begin{matrix} y_1\\\\ y_2\\\\. \\\\. \\\\ y_i \\\\. \\\\. \\\\ y_m \\end{matrix} \\right)_{(m,1)}$$\n",
    "\n",
    "Итак, мы представляем функцию-гипотезу в векторизованном виде: $$\\mathbf{ h_\\theta{(x)} = X\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn позволяет визуализировать линии регресии с помощью lmplot https://seaborn.pydata.org/tutorial/regression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь (Loss function)\n",
    "\n",
    "\n",
    "Функция потерь измеряет степень ошибки модели с точки зрения её способности оценить взаимосвязь между $x$ и $y$.\n",
    "Ведь нужен способ измерить, насколько наша предсказанная прямая ошибается. Мы не можем просто посчитать разницы, потому что некоторые ошибки будут положительными, некоторые отрицательными, и в сумме они дадут ноль, даже если ошибки огромны.\n",
    "\n",
    "Существует много способов определить функцию потерь. Наиболее популярный - метод наименьших квадратов\n",
    "\n",
    "1. Для каждого объекта считает разницу между реальным значением целевой переменной и предсказанием\n",
    "Ошибка = Y_real - Y_pred\n",
    "\n",
    "2. Возводим каждую ошибку в квадрат. Это решает проблему с знаками (все числа становятся положительными) и сильно штрафует модель за большие ошибки.\n",
    "Квадрат ошибки = (Y_real - Y_pred)²\n",
    "\n",
    "3. Складываем все квадраты ошибок для всех объектов. \n",
    "\n",
    "4. Делим на количество объектов в датасете, чтобы получить среднее значение.\n",
    "\n",
    "4 шага дают функцию потерь, которая покажет насколько \"не точна\" наша модель\n",
    "\n",
    "Для нашей задачи\n",
    "$$L(a, b) = (1/n) * Σ [ (Yreal_i - (\\theta_0+\\theta_1 age + \\theta_2 sex + \\theta_3 bmi + \\theta_4 children + \\theta_5 smoker + \\theta_6))² ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимум функции потерь - наша цель\n",
    "\n",
    "## Градиентный спуск\n",
    "\n",
    "Как известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента\n",
    "\n",
    "<img src=\"data/descent.png\"></img>\n",
    "\n",
    "<img src=\"data/descent2.png\"></img>\n",
    "\n",
    "<img src=\"data/descent3.png\"></img>\n",
    "\n",
    "<img src=\"data/descent4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск\n",
    "\n",
    "На каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке. Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация\n",
    "\n",
    "<img src=\"data/regular1.png\"></img>\n",
    "\n",
    "К чему это приводит переобучение?\n",
    "\n",
    "- Наша модель становится слишком сложной. Она уже не прямая линия, а \"зазубренная\" кривая, которая изгибается, чтобы пройти через каждую точку.\n",
    "\n",
    "Как понять что идет переобучение\n",
    "- На тренировочных данных (на тех примерах, что мы ей показали) она ошибается очень мало (функция потерь почти нулевая). Но на новых данных она делает большие ошибки. \n",
    "- большие коэффициенты (таким образом она \"объясняет шум в данных\")\n",
    "- мало данных при многих признаках\n",
    "\n",
    "Регуляризация - штрафное слагаемое функции потерь \n",
    "\n",
    "**Новая_Функция_Потерь = Старая_Функция_Потерь + λ * Штраф(Веса)**\n",
    "λ — коэффициент регуляризации. Он определяет, насколько сильно мы штрафуем модель\n",
    "\n",
    "Варианты регуляризации: L1 (Lasso) vs L2 (Ridge)\n",
    "\n",
    "Вариант 1: L2-регуляризация (Rgeression) — \"Штраф за слишком быструю езду\"\n",
    "\n",
    "Как считается штраф? Штраф = a₁² + a₂² + ... + aₙ² (Сумма квадратов всех весов).\n",
    "Как работает?\n",
    "\n",
    "Она не обнуляет веса, а равномерно уменьшает их все, сжимая модель.\n",
    "\n",
    "Когда использовать? Когда все признаки в той или иной степени полезны для предсказания.\n",
    "Вариант 2: L1-регуляризация (Lasso-регрессия) — \"Отбор самых важных признаков\"\n",
    "\n",
    "Как считается штраф? Штраф = |a₁| + |a₂| + ... + |aₙ| (Сумма модулей всех весов).\n",
    "Как работает?\n",
    "\n",
    "Она имеет очень полезное свойство: она обнуляет веса менее важных признаков.\n",
    "Когда использовать? Когда большинство из них бесполезны или избыточны. Lasso помогает упростить модель и отобрать только самое главное.\n",
    "Сравнение на примере:\n",
    "\n",
    "Допустим, у нас есть три признака для предсказания цены квартиры:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/regular2.png\"></img>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 13720,
     "sourceId": 18513,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 9432,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
